# LLM RNN CLI â€” Text Generation via Docker Compose âœ¨

Cette page explique comment gÃ©nÃ©rer du texte avec le modÃ¨le RNN depuis le conteneur llm, via la CLI llm_rnn.generate. ğŸ§ ğŸ³

---

Sommaire
- TL;DR rapide âš¡
- Commandes complÃ¨tes (PowerShell/Bash) ğŸ–¥ï¸
- Ce que fait la commande ğŸ”
- Options de la CLI âš™ï¸
- Exemples utiles ğŸ§ª
- ExÃ©cuter hors Docker (avancÃ©) ğŸš€
- Fichiers et volumes ğŸ“
- DÃ©pannage ğŸ›Ÿ
- API et mÃ©triques ğŸŒğŸ“ˆ
- Annexes ğŸ“š

---

TL;DR âš¡

PowerShell (Windows)

```powershell
PS C:\Users\vincent\huginn> docker compose exec llm /opt/venv/bin/python -m llm_rnn.generate `
  --ckpt /ckpts/rnn.pt `
  --seed "Le soleil se lÃ¨ve sur les collines et" `
  --chars 200 --temp 0.8 --top-p 0.9
```

Bash (macOS/Linux)

```bash
docker compose exec llm /opt/venv/bin/python -m llm_rnn.generate \
  --ckpt /ckpts/rnn.pt \
  --seed "Le soleil se lÃ¨ve sur les collines et" \
  --chars 200 --temp 0.8 --top-p 0.9
```

Ce que Ã§a fait ğŸ”
- docker compose exec llm â€¦ exÃ©cute la commande dans le conteneur llm (voir compose.yaml).
- /opt/venv/bin/python -m llm_rnn.generate lance lâ€™entrÃ©e CLI du package.
- --ckpt /ckpts/rnn.pt pointe vers le checkpoint montÃ© depuis ./ckpts de lâ€™hÃ´te.

Options de la CLI âš™ï¸
ImplÃ©mentationÂ : apps/python-llm/llm_rnn/generate.py
- --ckpt PATHÂ : chemin du checkpoint (.pt). DÃ©fautÂ : /ckpts/rnn.pt
- --seed TEXTÂ : texte dâ€™amorÃ§age (optionnel). DÃ©fautÂ : vide
- --chars INTÂ : nombre de caractÃ¨res gÃ©nÃ©rÃ©s. DÃ©fautÂ : 200
- --temp FLOATÂ : tempÃ©rature (plus haut = plus alÃ©atoire). DÃ©fautÂ : 0.9
- --top-k INTÂ : coupe topâ€‘k (optionnel). DÃ©fautÂ : None
- --top-p FLOATÂ : nucleus/topâ€‘p (optionnel). DÃ©fautÂ : None

Exemples utiles ğŸ§ª
1) Ã‰chantillon rapide (dÃ©fautsÂ : /ckpts/rnn.pt, 200 chars, temp=0.9)

```powershell
PS> docker compose exec llm /opt/venv/bin/python -m llm_rnn.generate --seed "Hello world"
```

2) Sortie plus dÃ©terministe (temp plus basse, sans nucleus)

```powershell
PS> docker compose exec llm /opt/venv/bin/python -m llm_rnn.generate `
  --seed "Once upon a time" `
  --chars 400 `
  --temp 0.6
```

3) Utiliser topâ€‘k au lieu de topâ€‘p

```bash
docker compose exec llm /opt/venv/bin/python -m llm_rnn.generate \
  --seed "Il Ã©tait une fois" \
  --chars 300 --temp 0.8 --top-k 50
```

ExÃ©cuter hors Docker (avancÃ©) ğŸš€
Si votre environnement local a Python + PyTorch et que le package est accessible (PYTHONPATH=/app/apps/python-llm dans lâ€™image), vous pouvez lancerÂ :

```bash
python -m llm_rnn.generate --ckpt ./ckpts/rnn.pt --seed "Example" --chars 200
```

Assurezâ€‘vous que llm_rnn (apps/python-llm) est sur le PYTHONPATH, ou installezâ€‘le en package si vous avez une configuration prÃ©vue.

Fichiers et volumes ğŸ“
- CheckpointÂ : ./ckpts/rnn.pt (hÃ´te) montÃ© en /ckpts/rnn.pt (conteneur). compose.yamlÂ : ./ckpts:/ckpts
- CodeÂ : ./apps/python-llm montÃ© en /app/apps/python-llm, avec PYTHONPATH=/app/apps/python-llm pour python -m llm_rnn.generate

DÃ©pannage ğŸ›Ÿ
- ModuleNotFoundError: No module named 'llm_rnn'
  - ExÃ©cutez dans le conteneur (docker compose exec llm â€¦) oÃ¹ PYTHONPATH est dÃ©fini.
  - En local, dÃ©finissez PYTHONPATH=apps/python-llm ou installez le package.
- FileNotFoundError: /ckpts/rnn.pt
  - VÃ©rifiez lâ€™existence de ./ckpts/rnn.pt et le volume ./ckpts:/ckpts dans compose.yaml.
  - Changez --ckpt si nÃ©cessaire (ex.Â : un chemin dans /app/â€¦).
- Sortie Â«Â bizarreÂ Â» ou caractÃ¨res inattendus
  - Baissez --temp vers 0.6â€“0.8.
  - Assurez la compatibilitÃ© tokenizer/checkpoint (byte/BPE). Le loader dÃ©tecte le type via la config sauvegardÃ©e.

API et mÃ©triques ğŸŒğŸ“ˆ
- HTTP APIÂ : le backend Symfony sâ€™attend Ã  http://llm:8008/generate/stream (voir apps/symfony-back/src/Infrastructure/LLM/PythonLLMClient.php). La CLI ciâ€‘dessus est pratique pour des tests directs.
- MÃ©triquesÂ : http://localhost:9108/metrics exposÃ© par services/llm/metrics_server.py (dans le conteneurÂ : /metrics sur 9108).

Annexes ğŸ“š
- EntrÃ©e CLIÂ : apps/python-llm/llm_rnn/generate.py
- ModÃ¨leÂ : apps/python-llm/llm_rnn/model.py (CharRNN + sampling)
- TokenizersÂ : apps/python-llm/llm_rnn/tokenizer.py et tokenizer_bpe.py

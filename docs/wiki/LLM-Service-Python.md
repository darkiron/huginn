# LLM Service (Python) üß†

Notes sur le service LLM Python. Pour les d√©tails approfondis, consulter aussi: ../../docs/services/python-llm.md

## R√¥le
- Fournit des capacit√©s de g√©n√©ration de texte (consomm√©es par le backend Symfony).
- Expose des m√©triques Prometheus pour l'observabilit√©.
- S'ex√©cute en local via Docker Compose pour une coh√©rence d'environnement.

## API HTTP attendue par le backend
- Endpoint: `POST /generate/stream`
- URL par d√©faut c√¥t√© backend: `http://llm:8008/generate/stream`
  - Configurable via `LLM_URL` (compose/env) ou param√®tre Symfony `llm_url`.
- Client c√¥t√© Symfony: `apps/symfony-back/src/Infrastructure/LLM/PythonLLMClient.php`
  - Utilise le streaming (HttpClientInterface avec `buffer: false`).

Remarque: l'impl√©mentation HTTP de l'endpoint n'est pas incluse dans les extraits visibles ici; assurez-vous que le service LLM r√©pond bien sur ce chemin si utilis√© en production.

## CLI ‚Äî G√©n√©ration rapide
- Voir [LLM RNN CLI](LLM-RNN-CLI) pour g√©n√©rer du texte directement dans le conteneur `llm`.
- Entr√©e: `python -m llm_rnn.generate` avec options `--ckpt`, `--seed`, `--chars`, `--temp`, `--top-k`, `--top-p`.

## Tokenization
- Modes pris en charge: byte-level (par d√©faut), legacy char-level, BPE.
- D√©tails: [Tokenization](Tokenization) (modes, heuristiques, encode/decode).

## M√©triques Prometheus
- Expos√©es par `services/llm/metrics_server.py` sur `/metrics`.
- Port par d√©faut: `METRICS_PORT=9108` (voir compose.yaml -> service `llm`).
- Exemple local: http://localhost:9108/metrics

## Supervisor (processus)
- Fichier: `services/llm/supervisor.py`
  - Lance le serveur de m√©triques dans un thread.
  - Maintient le conteneur vivant via une boucle idle.
- Commande compose: `python services/llm/supervisor.py`

## R√©pertoire mod√®le & checkpoints
- Volume: `./ckpts` (h√¥te) mont√© en `/ckpts` (conteneur).
- Chemin par d√©faut du checkpoint: `/ckpts/rnn.pt` (configurable via `CKPT_PATH`).

## Fichiers cl√©s (apps/python-llm)
- `llm_rnn/generate.py` ‚Äî CLI de g√©n√©ration (charge le ckpt, choisit le tokenizer, √©chantillonne via le mod√®le).
- `llm_rnn/model.py` ‚Äî d√©finition du mod√®le `CharRNN` et logique de sampling.
- `llm_rnn/tokenizer.py` ‚Äî tokenizer byte/legacy.
- `llm_rnn/tokenizer_bpe.py` ‚Äî tokenizer BPE (si ckpt entra√Æn√© avec BPE).

Voir aussi:
- [LLM RNN CLI](LLM-RNN-CLI)
- [Backend-Symfony](Backend-Symfony)
- [Architecture](Architecture)

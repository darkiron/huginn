# Tokenization üî§

Aper√ßu des strat√©gies de tokenisation utilis√©es par le service LLM Python. Source code: apps/python-llm/llm_rnn.

## Modes pris en charge
- Byte-level (par d√©faut) ‚Äî 0..255
  - Pr√©serve les accents/UTF‚Äë8 via encodage/d√©codage bytes.
  - Impl√©mentation: `llm_rnn/tokenizer.py` (`CharTokenizer` avec `mode="byte"`).
- Legacy char-level ‚Äî liste de caract√®res
  - Pour compat CKPT anciens; mappe chaque char √† un indice.
  - Hors vocabulaire: `?` si pr√©sent, sinon fallback 0.
- BPE ‚Äî (si le checkpoint est entra√Æn√© avec BPE)
  - Impl√©mentation: `llm_rnn/tokenizer_bpe.py` (requis par `tokenizer_kind="bpe"`).

## D√©tection du mode
- Dans `llm_rnn/generate.py`: le checkpoint charge `config["tokenizer_kind"]` ("bpe" | "byte" | "legacy").
- Sinon, heuristique via `chars`:
  - ints -> byte-level
  - str -> legacy char-level

## API encode/decode
- encode(str) -> List[int]
  - byte-level: UTF‚Äë8 -> bytes -> liste 0..255 (errors='replace')
  - legacy: conversion char->id avec fallback `?`/0
- decode(List[int]) -> str
  - byte-level: bytes(...).decode('utf-8', errors='replace')
  - legacy: join via itos avec fallback `?`

## Bonnes pratiques
- V√©rifier la compatibilit√© entre le checkpoint et la tokenizer (kind, vocab).
- En cas d'artefacts de d√©codage, baisser la temp√©rature (`--temp`) et confirmer le mode.

Voir aussi:
- [LLM RNN CLI](LLM-RNN-CLI)
- [LLM-Service-Python](LLM-Service-Python)